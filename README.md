# Privacy Risks of Large Language Models

This repository is a collection of links to papers and code repositories used in our survey on privacy risks of LLMs.

## Table of Contents

- [Privacy Risks of LLMs](#privacy-risks-of-large-language-models)
  - [Memorization](#memorization)
  - [Privacy Attacks](#privacy-attacks)
  - [Private LLMs](#private-llms)
  - [Unlearning](#unlearning)
  - [Copyright](#copyright)


## Citation

```
TODO: Fill in once on Arxiv
```

# Memorization
![image info](figs/carlini_extraction.png)
Image from [Carlini 2020](https://arxiv.org/abs/2012.07805)

| **Paper Title** | **Year** | **Author**        | **Code** |
| --------------- |:--------:|-------------------| :----: |
| [Emergent and Predictable Memorization in Large Language Models](https://arxiv.org/abs/2304.11158) |   2023   | Biderman et al.   | [[Code]](https://github.com/EleutherAI/pythia) |
| [The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks](https://arxiv.org/abs/1802.08232) |   2019   | Carlini et al.    |  |
| [Quantifying Memorization Across Neural Language Models](https://arxiv.org/abs/2202.07646) |   2023   | Carlini et al.    |  |
| [Does Learning Require Memorization? A Short Tale about a Long Tail](https://arxiv.org/abs/1906.05271) |   2020   | Feldman et al.    |  |
| [Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy](https://arxiv.org/abs/2210.17546) |   2023   | Ippolito et al.   |  |
| [Measuring Forgetting of Memorized Training Examples](https://arxiv.org/abs/2207.00099) |   2023   | Jagielski et al.  |  |
| [Deduplicating Training Data Mitigates Privacy Risks in Language Models](https://arxiv.org/abs/2202.06539) |   2022   | Kandpal et al.    |  |
| [How BPE Affects Memorization in Transformers](https://arxiv.org/abs/2110.02782) |   2021   | Kharitonov et al. |  |
| [Deduplicating Training Data Makes Language Models Better](https://arxiv.org/abs/2107.06499) |   2022   | Lee et al.        | [[Code]](https://github.com/google-research/deduplicate-text-datasets) |
| [How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN](https://arxiv.org/abs/2111.09509) |   2021   | McCoy et al.      | [[Code]](https://github.com/tommccoy1/raven) |
| [Training Production Language Models without Memorizing User Data](https://arxiv.org/abs/2009.10031) |   2020   | Ramaswamy et al.  |  |
| [Understanding Unintended Memorization in Federated Learning](https://arxiv.org/abs/2006.07490) |   2020   | Thakkar et al.    |  |
| [Investigating the Impact of Pre-trained Word Embeddings on Memorization in Neural Networks](https://www.researchgate.net/publication/344105641_Investigating_the_Impact_of_Pre-trained_Word_Embeddings_on_Memorization_in_Neural_Networks) |   2020   | Thomas et al.     |  |
| [Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models](https://arxiv.org/abs/2205.10770) |   2022   | Tirumala et al.   |  |
| [Understanding deep learning requires rethinking generalization](https://arxiv.org/abs/1611.03530) |   2017   | Zhang et al.      |  |
| [Counterfactual Memorization in Neural Language Models](https://arxiv.org/abs/2112.12938) |   2021   | Zhang et al.      |  |
| [Provably Confidential Language Modelling](https://arxiv.org/abs/2205.01863) |   2022   | Zhao et al.       | [[Code]](https://github.com/XuandongZhao/CRT) |


# Privacy Attacks
![image info](figs/tindall_mia.png)
Image from [Tindall](https://gab41.lab41.org/membership-inference-attacks-on-neural-networks-c9dee3db67da)

# Private LLMs
![image info](figs/google_fl.png)
Image from [Google AI Blog](https://blog.research.google/2017/04/federated-learning-collaborative.html)

| **Paper Title**                                                                                           | **Year** | **Author**  | **Code** |
|-----------------------------------------------------------------------------------------------------------|:--------:|-------------| :----: |
| [Knowledge Unlearning for Mmitigating Privacy Risks in Language Models](https://arxiv.org/pdf/2210.01504) |   2022   | Jang et al. | [[Code]](https://github.com/joeljang/knowledge-unlearning) |



# Unlearning
![image info](figs/felps_unlearning.png)
Image from [Felps 2020](https://www.researchgate.net/publication/346879997_Class_Clown_Data_Redaction_in_Machine_Unlearning_at_Enterprise_Scale)

| **Paper Title**                                                                                           | **Year** | **Author**  | **Code** |
|-----------------------------------------------------------------------------------------------------------|:--------:|-------------| :----: |
| [Knowledge Unlearning for Mmitigating Privacy Risks in Language Models](https://arxiv.org/pdf/2210.01504) |   2022   | Jang et al. | [[Code]](https://github.com/joeljang/knowledge-unlearning) |


# Copyright
![image info](figs/copyright_hobbit.png)
Custom Image
